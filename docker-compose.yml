# This file is kept for backward compatibility
# For local development, use: docker-compose -f docker-compose.dev.yml
# For production, use: docker-compose -f docker-compose.prod.yml

services:
  app:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: chitalishta_ai_chat_api
    ports:
      - "8000:8000"
    environment:
      # Database connection
      DATABASE_URL: postgresql://root:root@db:5432/chitalishta_db
      # Chroma vector store
      CHROMA_PERSIST_DIRECTORY: /app/chroma_db
      CHROMA_COLLECTION_NAME: chitalishta_documents
      # Embedding configuration (set in .env or override)
      EMBEDDING_PROVIDER: ${EMBEDDING_PROVIDER:-openai}
      OPENAI_API_KEY: ${OPENAI_API_KEY:-}
      OPENAI_EMBEDDING_MODEL: ${OPENAI_EMBEDDING_MODEL:-text-embedding-3-small}
      HUGGINGFACE_MODEL_NAME: ${HUGGINGFACE_MODEL_NAME:-intfloat/multilingual-e5-base}
      # LLM configuration (set in .env or override)
      LLM_PROVIDER: ${LLM_PROVIDER:-openai}
      LLM_PROVIDER_CLASSIFICATION: ${LLM_PROVIDER_CLASSIFICATION:-}
      LLM_PROVIDER_GENERATION: ${LLM_PROVIDER_GENERATION:-}
      LLM_PROVIDER_SYNTHESIS: ${LLM_PROVIDER_SYNTHESIS:-}
      OPENAI_CHAT_MODEL: ${OPENAI_CHAT_MODEL:-gpt-4o-mini}
      HUGGINGFACE_LLM_MODEL: ${HUGGINGFACE_LLM_MODEL:-HuggingFaceH4/zephyr-7b-beta}
      # TGI configuration
      TGI_BASE_URL: ${TGI_BASE_URL:-http://tgi:80/v1}
      TGI_MODEL_NAME: ${TGI_MODEL_NAME:-google/gemma-2b-it}
      TGI_TIMEOUT: ${TGI_TIMEOUT:-30}
      TGI_ENABLED: ${TGI_ENABLED:-true}
      # Hugging Face token (for gated models)
      HUGGING_FACE_HUB_TOKEN: ${HUGGING_FACE_HUB_TOKEN:-}
    volumes:
      # Persist Chroma DB data
      - chroma_db_data:/app/chroma_db
      # Mount documents directory (for analysis document ingestion)
      - ./documents:/app/documents:ro
    depends_on:
      db:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - chitalishta_network

  db:
    image: postgres:latest
    container_name: chitalishta_db_ai_chat
    environment:
      POSTGRES_USER: root
      POSTGRES_PASSWORD: root
      POSTGRES_DB: chitalishta_db
    ports:
      - "5434:5432"
    volumes:
      # PostgreSQL 18+ requires mounting at /var/lib/postgresql instead of /var/lib/postgresql/data
      - chitalishta-crawler_pg_data_chitalishta:/var/lib/postgresql
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U root || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - chitalishta_network

  test_db:
    image: postgres:latest
    container_name: chitalishta_test_db
    environment:
      POSTGRES_USER: root
      POSTGRES_PASSWORD: root
      POSTGRES_DB: chitalishta_test_db
    ports:
      - "5435:5432"
    volumes:
      # PostgreSQL 18+ requires mounting at /var/lib/postgresql instead of /var/lib/postgresql/data
      - pg_data_chitalishta_test:/var/lib/postgresql
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U root || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - chitalishta_network

  # tgi:
  #   image: ghcr.io/huggingface/text-generation-inference:latest
  #   platform: linux/amd64  # Required for ARM64 Mac compatibility (uses x86_64 emulation)
  #   container_name: chitalishta_tgi
  #   shm_size: 1g  # Shared memory for model loading
  #   ports:
  #     - "8080:80"  # TGI OpenAI-compatible API endpoint
  #   volumes:
  #     - tgi_model_cache:/data
  #   environment:
  #     # Hugging Face token for accessing gated models (e.g., google/gemma-2b-it)
  #     # Get your token from: https://huggingface.co/settings/tokens
  #     # Add it to your .env file as: HUGGING_FACE_HUB_TOKEN=your_token_here
  #     - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN:-}
  #   command:
  #     - --model-id
  #     - google/gemma-2b-it
  #     - --port
  #     - "80"
  #     - --max-input-length
  #     - "1536"
  #     - --max-total-tokens
  #     - "2048"
  #     - --disable-custom-kernels  # Required for CPU-only inference
  #   deploy:
  #     resources:
  #       limits:
  #         memory: 6G  # Allocate 6GB RAM for Gemma-2b (smaller model)
  #       reservations:
  #         memory: 4G
  #   healthcheck:
  #     test: ["CMD-SHELL", "curl -f http://localhost:80/health || exit 1"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 5
  #     start_period: 180s  # Give TGI time to download and load model on first start (3 minutes for smaller model)
  #   restart: unless-stopped

volumes:
  # Database volume (changed from external to regular volume since it doesn't exist)
  chitalishta-crawler_pg_data_chitalishta:
  pg_data_chitalishta_test:
  # Chroma DB data persistence
  chroma_db_data:
  # tgi_model_cache:

networks:
  chitalishta_network:
    driver: bridge

